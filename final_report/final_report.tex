\documentclass[a4paper,11pt,english]{article}
\usepackage[english]{babel} 
\usepackage[T1]{fontenc}    
\usepackage{lmodern}
\usepackage[utf8]{inputenc} 
\usepackage{graphicx}       
\usepackage{hyperref}
\usepackage{todonotes}
\usepackage{amsmath}
\usepackage{siunitx}

\begin{document}

\title{Active strategies for object discovery}
\author{Phil Bradfield \and Jan Fabian Schmid}

\maketitle 

\section{Introduction}
\label{Introduction}

\input{sections/intro}

\section{Theoretical background}
\label{Theoretical_background}
%Theoretical background, which provides the descriptions on what existing theories and ideas that are related to your problem statement. The purpose of this section is to ensure that you are knowledgeable about the related key concepts, theories and models. Do not copy your text from the intermediate report. Revise and rewrite according to the feedback you got and what the actual outcome of the project was.

In this section we present some of the concepts and methods that we utilize in our system.

\subsection{Saliency-based object discovery}
\label{Theoretical_background:Saliency-based_object_discovery}
To find objects in the robots current view, we use a saliency-based object discovery system.
Such a system has been introduced by García et al. \cite{garcia2015saliency}.
An overview of the method can be seen in Figure \ref{fig:2Dobject_discovery}.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1\textwidth]{src/saliency_object_detection.png}
		\caption{ Overview of the object candidate generation process \cite{garcia2015saliency}}
		\label{fig:2Dobject_discovery}
	\end{center}
\end{figure}

The method requires a colour and depth image of the same scene.
While it would work with colour information only experiments of the authors have shown that the combination of both achieves the best results.
The colour image is processed in two different ways: 
Using the Felzenszwalb and Huttenlocher algorithm \cite{felzenszwalb2004efficient} the image is segmented into small patches of similar colour.
Simultaneously a saliency map is computed. This is done with the VOCUS2 system developed by Frintrop et al. \cite{frintrop2015traditional}.
The general idea of VOCUS2 is to compute for each pixel a value quantifying its contrast to its proximate image area.
To generate object candidates the information of saliency map and colour segmentation are used together.
Seeded region growing is performed on the local maxima in the saliency map.
This means that starting from a local maxima neighboring pixels are iteratively grouped together into one region if their saliency value exceeds a certain percentage of the saliency value of the local maximum.
The result is a list of salient blobs in the saliency map.
For each salient blob one colour object candidate is created. It contains the pixels of all colour segments that overlap to at least $30\%$ with the pixels of the salient blob.
The depth candidates are generated analogous to the colour candidates, using the same salient blobs as a basis for the object candidates, but a surface clustering method is used as segmentation algorithm. It divides the image into continuous planes.
Afterwards, colour and depth candidates are merged into one set a ranking of proposed object candidates in terms of \glqq{}objectiveness\grqq{} takes place.

With more than 80\% recall and up to 50\% precision the saliency-based object discovery method performs comparable or better than other state-of-the-art object detection systems tested in the experiments of García et al.

\subsection{Frontier-based exploration}
\label{Theoretical_background:Frontier-based_exploration}
A method for autonomous exploration of unknown  environments has been introduced 1997 by Yamauchi \cite{yamauchi1997frontier}.
He describes the task that he wants to solve as follows: \glqq{}The central question in exploration is: \textit{Given what you know about the world, where should you move to gain as much  new  information  as  possible?}\grqq{}.
This is the same problem we have to solve for our object seeking robot.
Figure \ref{fig:frontier} presents how frontier-based exploration works.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=.85\textwidth]{src/frontier_exploration.png}
		\caption{ Processing steps of frontier-based exploration \cite{yamauchi1997frontier}}
		\label{fig:frontier}
	\end{center}
\end{figure}

On the left (a) of the figure a possible map of the environment is depicted. Black dots represent unexplored cells of the map, stronger black dots mark found obstacles and white space is the available free space in which the robot should be able to navigate.
All cells on the border between free and unexplored space are marked in (b) as frontier edge segments.
Neighboring frontier edge segments are grouped together and if the contiguous space of the group is roughly larger than the robot, the group defines a frontier. The centroid of each frontier is represented by a crosshair in (c).
The robot will then navigate to the closest accessible frontier centroid and thereby explore the environment.

Yamauchi performed successful experiments using a Nomad 200 mobile robot in indoor environments.

\subsection{Sampling-based exploration}
\label{Theoretical_background:Sampling-based_exploration}
Sampling-based exploration is another way of finding a next best viewpoint to explore an initially unknown environment.
The general idea is to randomly sample the accessible area of the robot for possible viewpoints.
For each viewpoint a information gain score is computed, which depends on the visible area at the particular viewpoint and the distance of this viewpoint to the current robot position.

Such a system has been introduced by Surmann et al. \cite{surmann2003autonomous}.
For their system they use laser scanner data. From this data they detect obstacles, free space and unknown space.
The borders between free space and unknown space are called unseen lines.
Next best viewpoint candidates are randomly sampled from the free space.
For each of these candidates the number of visible unseen lines is computed, which determines its information gain.
The final score for each candidate is computed by multiplying the information gain with a punishing factor that depends on the euclidean distance to the current robot position and the angle that the robot would have to rotate for the view.
The next goal is then chosen as the viewpoint candidate with highest overall score.

%\subsection{Particle filter SLAM}
%\subsection{Euclidean point cloud clustering}
\subsection{IoR mechanism to guide attention}
\label{Theoretical_background:IoR mechanism_to_guide_attention}
An inhibition of return (IoR) mechanism can be used in computer vision to avoid spending attention on the same region for longer than necessary to process the information of that region.
Such a method is used in the object detection system by García et al. \cite{garcia2015saliency} to avoid the computation of the same object candidates in consecutive frames of a video sequence.
The system keeps track of previously considered object candidates through an 3D IoR map, which is a model of the environment built through a reconstruction algorithm (KinectFusion \cite{newcombe2011kinectfusion}) from the depth image of the RGB-D camera, where information about IoR is stored in the voxels of the map.
To update the IoR information in the map the current 2D camera frame is projected into the 3D map.
That way, voxels corresponding to the attended object candidates are found.
The IoR weight for each of these voxels is increased and if a certain threshold is reached an IoR flag is activated, signaling that pixels in the camera frame corresponding to this voxel should be inhibited.
For each frame that a voxel is not attended its IoR weight decreases and if it reaches zero again the IoR flag is deactivated.

The integration of this 3D IoR mechanism allowed García et al. to find most of the visible objects in a video while considering only a few object candidates per frame.

\subsection{Building a 3D map with octree data structure}
\label{Theoretical_background:Building_a_3Dmap_with_octree_data_structure}

\input{sections/octrees}

\section{Implementation}
\label{Implementation}
%Implementation, which connects with the section on theoretical background and describes how the required parts were implemented, and how the overall system is constructed.

The general structure of our system has been introduced in Section \ref{Introduction} already.
We have mostly developed our system inside a physics simulation (Gazebo \footnote{\url{http://gazebosim.org/}}), however we transferred the system with slight adjustments onto a real Pioneer robot as well.
In the following we will explain how the different parts in Figure \ref{fig:overview} have been implemented. If the simulated and real version are diverging we will point that out in the respective system part description.
The system is fully developed with the Robot Operating System (ROS \footnote{\url{http://www.ros.org/}}) in the Indigo version, therefore each part of the system consists of one or multiple ROS nodes and services.
The code for our system can be found online \footnote{\url{https://github.com/JanFabianSchmid/NBV-planning-for-object-detection}}.

\subsection{Mobile platform}
As mobile platform we used a Pioneer P3DX. This robot is providing a first estimate for its own position in form of odometry based on the previous motor control. Mounted onto the platform is a Kinect camera \footnote{\url{https://developer.microsoft.com/en-us/windows/kinect}}. In case of the real version we put the camera on top of the platform, in the physics simulation we are free of physical bounds and put the camera slightly lower in front of the robot to accommodate for our experimental setup in which objects are lying scattered on the floor.

\subsection{RGB-D camera}
The Kinect camera is directly providing a rectified version of the colour and depth image, which aligns the two frames, therefore pixel coordinates both images are corresponding to each other (\textbf{TODO is this correct?!}).
Also a point cloud build from the fused information of depth and colour image is directly provided as a ROS topic through the respective camera driver for simulation and real world.

\subsection{Laserscan}
To simulate a laser scanner we use the ros package \texttt{pointcloud\_to\_laserscan} \footnote{\url{http://wiki.ros.org/pointcloud_to_laserscan}}.
It receives the current point cloud from the RGB-D camera and samples the point cloud between a specified minimal and maximal height.
The closest point for each vertical line is put out to form one horizontal line which is our simulated laser scan.

\subsection{SLAM}
To perform simultaneous localization and mapping we use a ROS wrapper \footnote{\url{http://wiki.ros.org/gmapping}} for OpenSlam's GMapping algorithm \cite{grisetti2007}, which is an implementation of Fast SLAM.
GMapping performs laser-based SLAM using a particle filter.
It requires laser scan data and the estimated current robot position and provides a 2D occupancy grid map and the transform of the robot's pose into the map frame.
The occupancy grid map consists of cells of specified size.
Each cell can take one of four possible values.
The cell is thereby marked as obstacle, free space, unexplored space or as inscribed inflated obstacle, which means that this cell is not accessible by the robot since according to its footprint it would be in collision with an obstacle.

\subsection{Generation of object proposals}
The generation of object proposals is a complex part of the system that consists of five separate processing steps that we describe individually.

\subsubsection{2D Object candidate generation}
The first step in our method for object proposal generation is to find 2D object proposals in the current colour camera frame.
To achieve this we use the framework of García et al. presented in Section \ref{Theoretical_background:Saliency-based_object_discovery}.
We are currently only using the colour candidates. The object proposals contain usually most of the objects in the image and are of good quality, therefore we left the integration of the additional depth candidates for possible future work.
In Figure \ref{fig:object_generation} an example for the generation of object proposal is depicted.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1\textwidth]{src/object_generation.png}
		\caption{Left: Current camera image. Middle: First object proposal. Right: Second object proposal.}
		\label{fig:object_generation}
	\end{center}
\end{figure}

In our implementation of the framework of García et al. we start by computing a saliency map using VOCUS2 \cite{frintrop2015traditional}.
The colour segmentation is done using a C++ implementation of the Felzenszwalb and Huttenlocher algorithm \cite{felzenszwalb2004efficient} by Christoffer Holmstedt \footnote{\url{https://github.com/christofferholmstedt/opencv-wrapper-egbis}}.
Starting with the pixel with highest saliency, we perform seeded region growing where iteratively neighboring pixels are appended to the current salient region if their saliency value is at least 65\% of the value from the starting pixel.
In order to only keep actually interesting regions, a salient region is only kept for further processing if it has at least a mean saliency value of 130 (from a maximum of 255).
In order to prevent a large amount of strongly overlapping salient blobs, we do the following: For each considered saliency maximum the pixels from a seeded region growing, now using minimum similarity of 90\%, are then no longer considered as potential starting points for further salient regions.
Still some  of the salient regions will have large overlap, if the overlap is greater than 50\% only the one with higher saliency score is kept.
Where the saliency score is calculated as mean saliency times square root of the area of the corresponding region.
All of the remaining salient blobs will now correspond to one object proposal.
At this point the segments from the Felzenszwalb and Huttenlocher algorithm are used.
For each salient blob we find the corresponding segments that have least 30\% overlap.
The sum of these segments is the region of the object proposal for this particular salient blob.

\subsubsection{Building the proposal point cloud}
\label{sssec:building_proposal_point_cloud}

The 2D object proposals are next projected into 3D.
For each pixel which is part of the 2D proposal, we use the known geometry of the camera to project a ray from the Kinect's optical centre through the centre of the pixel and out into the world.
The value from the corresponding pixel in the Kinect's rectified depth map tells us how far along this ray the nearest surface is, thus giving us a point in 3D space which lies on the surface of our object candidate.

\subsubsection{Clustering of the proposal point cloud}
One problem with our approach of finding object proposals in the 2D image with subsequent projection into the third dimension using the available depth information is that imprecise or incorrect 2D object boundaries lead to distorted 3D object proposals.
For example it happens that larger parts of a wall far behind an object or parts of the floor are mistaken as part of the object.
One anomaly that occurred very often for us can be seen in Figure  \ref{fig:rays}.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.5\textwidth]{src/rays2.png}
		\caption{Large parts of the ball are correctly identified; however, some additional voxels at the right side behind the ball are mistaken as belonging to the object.}
		\label{fig:rays}
	\end{center}
\end{figure}

To avoid this behavior we added an additional step. 
The obtained proposal point cloud is clustered using PCLs Euclidean Cluster Extraction method.
This is a simple clustering algorithm.
The user defines a distance threshold.
All points that are closer as the distance threshold will end up in the same cluster.
For the further processing we use only the cluster with the closest point to the camera that has at least 50 points.
This is removing some of the anomalies, because the wrong parts of the object proposal will mostly be behind the actual object.

\input{sections/proposal_generation_3d}
% \subsubsection{Projection of point cloud into map}

% \subsubsection{Merging and handling of candidates in octomap}

\subsection{NBV planning}
In our current system five different next best view algorithms are available, which we will describe in their individual parts.
The goal of each of these algorithms is to find a next view point with high information gain.
All our methods use the occupancy grid map from GMapping for their computations.
A view consists of one cell of the map as goal position and a yaw orientation value.

\subsubsection{Random NBV}
Random NBV is the baseline method which we can use to assess the improvement through more sophisticated NBV methods.
For this method all currently accessible free space cells of the map are considered as next view position.
One of these cells is randomly selected and forms together with a random orientation the random NBV.

\subsubsection{Frontier-based exploration (\texttt{FBE})}
Our NBV method based on frontier exploration, which has been introduced in Section \ref{Theoretical_background:Frontier-based_exploration}, is performing frontier exploration as it has been implemented in the \texttt{frontier\_exploration} ROS package \footnote{\url{http://wiki.ros.org/frontier_exploration}}.
The next best view computed at a certain start position is here the centroid of the closest frontier with an orientation that corresponds to the continued line between the start position and the centroid of the closest frontier.
One essential flaw of this NBV method is that it works only until the environment has been completely discovered.

\subsubsection{Sampling-based exploration using obstacles (\texttt{SBE(Obs)})}
For this NBV method we use sampling-based exploration as described in Section \ref{Theoretical_background:Sampling-based_exploration}.
Here all accessible free space cells closer than $2$ meters and further than $20$ centimeters are considered as potential next best view position.
For each of these cells a random orientation is selected and all cells are found which are closer than $3$ meters and can be seen when the robot is looking in the selected orientation at this particular cell. This considers obstacles, therefore cells behind obstacles do not count as visible. %This is done by computing the part of a $3$ meter radius circle with center point corresponding to the selected orientation and an total angle that corresponds to the camera opening angle. Subsequently, for each pixel of the edge 
Each visible cell can contribute to the information gain at the potential viewpoint.
If a visible cell is part of the unexplored space or is an obstacle the information gain increases. Unexplored cells in front of an obstacle contribute an additional information gain bonus.
The idea is here that obstacles might correspond to objects and are therefore worth looking at.
Furthermore, unexplored space might contain undiscovered obstacles and objects, and an unexplored cell directly besides an obstacle has a high probability to be also part of this obstacle.
For each considered accessible free space cell we compute the sum of the information gain values of the visible cells considering the random orientation at the particular cell and multiply this value with a punishing factor depending on the distance of the cell to the current robot position.
Overall the information gain score $S$ is computed as:
\begin{align*}
 S &= (\mbox{sum of information gain})\cdot exp(-d\cdot \mbox{distance})
\end{align*}
Where $d$ is a constant that can be used to adjust how strongly distance is punished. We set $d$ to $0.05$ as it has been done in \cite{surmann2003autonomous}.
The NBV is then selected as the cell with corresponding chosen orientation that has the highest information gain score.

One problem that can occur for this NBV method is that the robot is moving close to a group of objects and the robot gets stuck continuously looking at this group of objects since they are close and looking at them is awarded with a lot of information gain score.
This is partly a desired behavior since multiple viewpoints on the same objects are necessary to increase the object proposal quality. However, if the robot gets stuck he will not find all objects distributed in the room.
To avoid getting stuck for too long, we implemented an inhibition of return (IoR) behavior. This concept has been introduced in Section \ref{Theoretical_background:IoR mechanism_to_guide_attention}.
We keep track of an IoR map that is similar to the occupancy grid map.
For each viewpoint taken, an the value in the IoR map for the obstacle cells the robot looked at is increased by $3$.
Obstacle cells that have not been looked at decrease their value in the IoR map by $1$.
For all future information gain scores obstacle cells with an IoR map value higher than $5$ provide no information gain.
Therefore, consecutive viewpoints taking a look at the same obstacle group have decreased information gain score, so that at some point the robot should move away from the group.

\subsubsection{Sampling-based exploration using obstacles and object candidates (\texttt{SBE(Obs+Cands)})}
This method is based on \texttt{SBE (obs)}.
The difference is that in addition to obstacles the current object candidates are considered as well for the information gain calculation.
Each visible cell at which currently an object is assumed to be will increase the information gain score and similar as before unexplored cells in front of object candidates will award an additional bonus information gain score.
The information gain $S_i$ for a cell $i$ with object candidate $j$ depends on the certainty $c_j$ into this candidate:
\begin{align*}
 S_i &= \mbox{inf\_object}\cdot 2^{-(c_j-1)}
\end{align*}
Where $\mbox{inf\_object}$ is the base value for object information gain. An object candidate with certainty $1$ will for example be awarded with $\mbox{inf\_object}$ information gain, a cell with candidate of certainty $2$, however, will only be awarded $\mbox{inf\_object}/2$.
The idea is here that object candidates with low certainty are worth more to look at, to increase the certainty.

\subsubsection{Combination of frontier and sampling-based exploration (\texttt{FBE plus})}
This method combines \texttt{FBE} and \texttt{SBE(obs)}.
Until the whole room is explored the frontier exploration method is used after that the sampling-based exploration takes over. 
The goal of this method is to overcome the limitation of frontier exploration, but exploit the positive behavior of frontier exploration to find a good sequence of views which allow to look at least once at all areas of the environment.

\subsection{Navigation}
For navigation we use the \texttt{move\_base} ROS package \footnote{\url{http://wiki.ros.org/move_base}}.
It requires the current robot position approximation, sensor transforms and sensor data and provides appropriate velocity commands to navigate the platform once a valid goal pose has been requested.

\section{Analysis}
\label{Analysis}
% Results, which contain the descriptions of the experiments conducted and presents their results.
In this section we describe the our experiments we conducted for the analysis of our system.
After explaining the experimental setups, we define the metrics we are measuring during system execution, introduce the experimental parameters and finally the results from our different experimental setups are presented.

\subsection{Experimental setups}
The experiments are done completely inside the simulation.
We built two different rooms which are used as environments, they can be seen in Figure \ref{fig:rooms}.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.49\textwidth]{src/clustered.png}
		\includegraphics[width=0.49\textwidth]{src/scattered.png}
		\caption{Our two tested environments. The left one will be called \textit{clustered objects} and the right one \textit{scattered objects}}
		\label{fig:rooms}
	\end{center}
\end{figure}

In both rooms are $12$ of the larger objects from the YCB object and model set \cite{calli2015benchmarking}.
On the left is an environment which contains two groups of three objects which are close to each other and therefore has also some larger areas without any object.
On the right is an alternative room with equally distributed objects. The distance between the objects is here always $1.5$ meters. Both rooms have a size of roughly $7$ by $7$ meters.
In each of our experiments the robot is spawned in the middle of the room and we define the area which is allowed to be used for view positions. This area is slightly smaller than the room, which means that the walls are not included.
For the frontier exploration method this area also defines what has to be explored.
For the sampling based methods information gain is only provided for cells inside the boundaries.

During our first experimental test runs, we noticed that the approximated position through GMapping are strongly diverging from the actual robot position.
This leads to a lot of problems. It means that the map will become imprecise, the navigation becomes difficult and the projection of object candidates into the map frame is strongly diverging from the actual object position.
The diverging object proposal projection is the largest problem for our system and prevents us from receiving reasonable results. Therefore, we are running our experiments in default using a \textit{cheat mode}.
In this mode the actual camera position is provided for the object proposal projection into the map frame, the other parts of the system still have to work with the approximated position.

\subsection{Metrics}
The evaluation of object proposals happens after each view taken by the robot.

One of the simplest things to measure is the expired time since the beginning of the experiment.
For our time measurement we are subtracting the time used for the evaluation.

To evaluate the object proposals we use the metrics recall and precision on an object level.
Recall measures the percentage of available objects found by the system and precision measures the percentage of object proposals that where correct.
To determine if an object has been correctly found, we determine the object proposal with the largest overlap to it and compute the intersection over union (IoU) between the ground truth object and the proposal.
If the IoU is above some threshold the proposal is considered as correct.
The overlap between proposal and ground truth is computed by counting the voxels in the octomap which are occupied by both and from the sum of the overlapping voxels we can compute the overlapping volume.

\subsection{Further experimental parameters}
Besides which of the two environment to use and activated or deactivated cheat mode there are a couple of more parameters to select or vary for the experiments:

\begin{itemize}
	\item \textbf{Available time:} The time until the experiment is aborted. This is set to 25 minutes for our experiments.
	\item \textbf{IoU threshold:} Determines the IoU value necessary for an proposal to be considered correct. The threshold is set to $0.1$ for our experiments.
	\item \textbf{Information gain parameters:} For the sampling-based exploration methods up to five values determining the information gain per cell have to be set. The default values are: 2 for unexplored cells, 10 for cells containing obstacles, 20 for unexplored cells in front of obstacles, 100 for objects (multiplied with the factor depending on the proposal certainty) and 30 for unexplored cells in front of objects.
	\item \textbf{Minimal required proposal certainty:} The minimal proposal certainty necessary for an proposal to be considered for the evaluation. All proposals with lower certainty will be ignored. This parameter is per default set to $1$.
\end{itemize}

\subsection{Results}
From simply observing the system in action, we observed that it was capable of generating object candidates which closely match to the ground truths (e.g. figure \ref{fig:good_results}).
In this section we attempt to quantify the system's performance using the results of our experiments.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.5\textwidth]{src/drill2.png}
		\caption{Example of object candidates generated by the system (the coloured voxels) which match closely to the ground truths (the black point clouds).}
		\label{fig:good_results}
	\end{center}
\end{figure}

Each experiment setting was run 25 times.
However, the system is crashing regularly. Most often because the robot thinks he is not able to move anymore.
In particular the \texttt{random NBV} method leads to many situations in which the robot can not move. Therefore, the \texttt{random NBV} method reached at most 13 views, while others were able to take up to 25 view poses.

At first we shall take a look at the development of recall and precision over the number of views taken.
This is depicted in Figure \ref{fig:recall_vs_nbv_count} and \ref{fig:precision_vs_nbv_count}.
Here we take the results from our room with scattered objects. Recall and precision curves are of similar shape but with slightly lower values overall for the room with some clustered objects.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1\textwidth]{src/Plots/recall_vs_nbv_count_scattered.png}
		\caption{Development of recall for all our NBV methods}
		\label{fig:recall_vs_nbv_count}
	\end{center}
\end{figure}

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1\textwidth]{src/Plots/precision_vs_nbv_count_scattered.png}
		\caption{Development of precision for all our NBV methods}
		\label{fig:precision_vs_nbv_count}
	\end{center}
\end{figure}

The best performing methods are \texttt{SBE(Obs)} and \texttt{FBE plus} which has the highest values for recall and precision.
They can reach recall values up to $0.75$ and have precision values between $0.25$ and $0.38$.
From the two graphs it becomes clear that all of the methods which somehow incorporate an information gain measure are more successful than the baseline method \texttt{random NBV}.
The \texttt{SBE(Obs+Cands)} is performing worse than than \texttt{SBE(Obs)}, our current assumption is that this is, because only about a quarter of objects are correct, thereby the robot gets more distracted from false object assumptions as he gets drawn to correct ones.
Variations between \texttt{FBE} and \texttt{FBE plus} and the beginning and between \texttt{FBE plus} and \texttt{SBE(Obs)} later occur through random fluctuations during the experiments.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1\textwidth]{src/Plots/setting__recall_vs_nbv_count.png}
		\caption{Recall values for different information gain parameter settings}
		\label{fig:recall_vs_nbv_count_settings}
	\end{center}
\end{figure}

In Figure \ref{fig:recall_vs_nbv_count_settings} different settings for the information gain parameters of our SBE methods are visualized in their effect on the recall. Precision is not strongly affected by the different settings.
The best performing settings are f) and e). In both of these setting information gain is only provided for cells containing obstacles, in e) a bonus is given for unexplored cells in front of obstacles (e) corresponds to \texttt{SBE (obs)}).
Setting a) is using the parameters from \texttt{SBE (obs+cands)} and b) is similar, but information gain through objects candidates is halved. c) is also related to a), but non of the bonuses for unexplored cells in front of obstacles or object candidates are given. These three settings perform similar, but much worse than the obstacle focused settings. This corresponds to the overall worse performing \texttt{SBE(Obs+Cands)} method.
The worst performing setting is d) where information gain is solely provided for cells containing object candidates. This fits to our previous results since this is the only setting that is not considering obstacles for the information gain calculations.

%\begin{figure}[h!]
%	\begin{center}
%		\includegraphics[width=1\textwidth]{src/Plots/setting__precision_vs_nbv_count.png}
%		\caption{Precision values for different information gain parameter settings}
%		\label{fig:precision_vs_nbv_count_settings}
%	\end{center}
%\end{figure}

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1\textwidth]{src/Plots/nbv_count_vs_time.png}
		\caption{Time used per view taken for all methods}
		\label{fig:nbv_count_vs_time}
	\end{center}
\end{figure}

In Figure \ref{fig:nbv_count_vs_time} the average time required to perform a certain number of views can be seen. 
Interestingly the \texttt{FBE} method is even faster than \texttt{random NBV}, this is because the random views can be far apart from each other, which leads Unsurprisingly, the \texttt{SBE} methods are the slowest solutions, because the computation of sophisticated information gain for many accessible cells is costly. In this graph the \texttt{FBE plus} method can show its advantage against the other \texttt{SBE} methods, because its  benefiting from the fast frontier exploration at the beginning.

\section{Conclusion}
\label{Conclusion}

\input{sections/conclusion}

\newpage
\bibliographystyle{plain}
\addcontentsline{toc}{section}{Bibliography}% Add to the TOC
\bibliography{bib}

\end{document}
