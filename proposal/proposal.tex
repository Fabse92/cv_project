\documentclass[a4paper,11pt,english]{article}
\usepackage[english]{babel} 
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc} 
\usepackage{graphicx}       
\usepackage{hyperref}      
\usepackage{todonotes}


\begin{document}

\title{Active strategies for object discovery}
\author{Phil Bradfield \and Jan Fabian Schmid}
	
\maketitle 

\section{Introduction}\label{intro}
In this project, we want to develop software for object discovery that is then implemented in a mobile robot.
The idea of object discovery is to find candidates for possible objects in the visual input.
A good object discovery algorithm is able to calculate a list of visual descriptors for the areas in the workspace that are most likely to correspond to discrete objects.
Each visual descriptor contains at least information about the spatial position and the shape of the object candidate.

The task of finding all objects in a scene without previous knowledge about the scene and the objects is still a largely unsolved problem \cite{garcia2013computational}.
Also the majority of research towards object detection considers only single images, as Atanasov et al. state \cite{atanasov2014nonmyopic}.
In a mobile system, however, we are not restricted to using only single images, which represent a single view on the environment.
Instead, we are able to move the robot to obtain different views on the same scene.
Therefore, object discovery on mobile systems introduces degrees of freedom that are not available in single images.
% Additional degrees of freedom may allow us to find better solutions to a problem, but at the same time often the problem gets more complicated.
In our case it is not only necessary to analyze a view of a scene for object candidates (1), but also to combine the data of multiple views (2) and to evaluate our current knowledge of both the scene and the surrounding environment in order to find the most interesting next view (3).

The first task (1) is the traditional single view object discovery problem. 
For the second task (2) the SLAM (\textbf{S}imultaneous \textbf{L}ocalization \textbf{A}nd \textbf{M}apping) problem has to be solved.
This problem occurs, because the robot starts with an unknown position in an unknown environment. 
Localization is then necessary to fuse the data from the current view with the data from previous views. 
Knowing the corresponding robot pose to the views allows to calculate the relative locations of data points from different views, which can be used to form a map \cite{surmann2003autonomous}. 
(3) is a problem from active sensing, which describes a class of problems where a decision making process is modeled that should lead a system to a state that corresponds to a low value of an uncertainty metric.
Our active sensing problem can be referred to as the \textbf{N}ext \textbf{B}est \textbf{V}iew (NBV) problem.
The NBV describes a pose that is near and accessible for the robot and promises high information gain \cite{surmann2003autonomous}.
For example, this problem has to be solved for the task of scanning all surfaces of an object \cite{pito1999solution}.

\subsection{Application scenarios}
Currently three scenarios of application with different complexity are planned.
\begin{itemize}	
	\item \textbf{Table scenario}: Objects of different size, shape and color are assembled on a table. Some objects are partially or completely occluded by other objects, therefore a single viewpoint is not sufficient to detect all objects. The mobile system has a RGB-D camera mounted at an appropriate hight to look on the table. It can move freely around the table to reach different viewpoints.
	\item \textbf{Pre-recorded images scenario}: This scenario is only an intermediate application for testing during the development of the system. We utilize the same setting as in the table scenario, but without an autonomous system. Using a RGB-D camera on a tripod, we take pictures from different viewpoints of the table. The system can use these images for object detection, computation of a NBV or movement of the system is not required.
	\item \textbf{Scattered objects scenario}: For the most complex scenario objects lie scattered on the floor. The mobile system can move in between the objects to take views on different areas of the scene. This scenario is especially more demanding on the abilities of computing the NBV and moving the robot to the desired viewpoints, which should allow us to examine the pros and cons of different NBV-algorithms.
\end{itemize} 

\subsection{Related work}\label{relatedwork}
Garc√≠a and Frintrop build a framework for 3D object detection by utilizing a visual attention system \cite{garcia2013computational}.
They work with a recorded video stream from an RGB-D camera. The color and depth stream are separately processed. 
The color stream is used to create so called proto-objects, which are areas with high probability of being part of an object. These proto-objects correspond to the peaks in the calculated saliency map. 
The areas of interest defined by the proto-objects are then segmented.
Afterwards, similar segments can be used to form an object candidate.
Simultaneously, the depth stream is used to build a map of the scene.
By projecting the object candidates into the 3D map they are able to label voxels as associated to a certain object in the scene.
An inhibition of return (IOR) mechanism for three dimensional scenes is used to allow the algorithm to focus on one salient region after the other.

We can use this framework as a starting point for our project.
As the framework is used for pre-recorded videos, the active sensing part that we need is missing.
Apart from that, we can use the described methods for object candidate creation and the fusion of data from multiple views.
For our project it may not be necessary to implement a 3D IOR map. This is because we don't have to use a continuously moving video stream to find object candidates. In our case it is possible to stop movement at our desired viewpoints and take a picture from which object candidates are determined. \medskip

Surmann et al. developed a system for autonomous digitalization of 3D indoor environments \cite{surmann2003autonomous}.
This system consists of three core modules: A mapping algorithm, a view planner and a navigator to move the robot to a desired goal.
A view planner is necessary, because many possible views promise only a very low information gain and the amount of views we can use is limited by time and energy constraints.
To compute the next best view they generate multiple horizontal 2D layers of their 3D map of the environment.
They use the Hough transform to find lines in the layer which represent the obstacles (e.g. walls) they already detected.
The lines are then connected to form an estimate of the layout of the room.
Then possible viewpoints are randomly generated.
For each viewpoint the information gain is calculated as a measure of how many unseen/assumed lines can be seen from it.
A compromise between traveling distance and information gain is then used to decide on the next viewpoint.

This approach might be a starting point for a good NBV algorithm for us. As we might not want the robot to explore a whole room, but only a specific area, we would have to find a way to only consider views on the relevant area.

\subsection{Relevance of our work for image processing in mobile systems}\label{relevanceimageprocessing}
Mobile systems are restricted in their computational processing power, therefore one important research question will be how much of the available data can be processed and which aspects are best to focus on. 
This is especially true for the task of solving the NBV problem, because active sensing algorithms can be arbitrarily complex.

The optimal solution to the NBV problem is most likely not available for applications in real environments, therefore we can only compute approximations.
For the very best approximations it might be necessary to consider huge amounts of data.
However, in our scenario it might be most important to just move somehow to an unused view. The difference between a good and the best NBV could be insignificant.
Finding a good balance between solving the NBV problem sufficiently well, and saving processing time and energy will likely be an important part of our work.

We will also have to find termination criteria which determine at which point no further investigation of the scene should be performed.
Therefore we could find a way to weigh up the energy cost for an operation against the possible benefit that it might provide to solve the task.
If there is no operation available for which the information gain is more valuable than the energy then the process stops.

\section{Working plan}\label{workingplan}
In this section we explain how the project is systematically divided into subtasks.
Therefore in each of the following subsections one subtask is examined in three aspects: What is achieved by solving the task, why is that necessary for our project and how can it be evaluated (if relevant for the subtask).

\subsection{Generating labeled test data}\label{subtask-datageneration}
We will create a benchmark dataset that contains multiple scenes with manually labeled objects.
Initial investigations may be possible using publicly available datasets such as those listed by Firman \cite{firman2016}, but at minimum we will need to build one or more scenes of our own in order to evaluate our systems on the physical robot system.

A ground truth is needed to evaluate the performance of different possible methods and parameters.
We will also be able to compare our solution against traditional single image object discovery approaches.

The main risk in this subtask is when we come to create and manually label our own dataset.
We must create a scene which is complex enough to be challenging while still remaining solvable, and the labeling must also be accurate in order for our evaluation to be meaningful.

\subsection{Perform object discovery on single images}\label{subtask-objectdiscovery}
The central aim of this subtask will be to determine object candidates from a single viewpoint.

From the analysis of the obtained object candidates the next viewpoint will be determined.
In the further processing the data of multiple viewpoints and the 3D map will be fused to get three dimensional object candidates.

The evaluation of our object discovery method for single images can be compared directly with other common approaches on the corresponding benchmark tests.

In this subtask we aim to implement the system developed by Garc\'{i}a and Frintrop, rather than implementing our own. This should keep the risk associated with this phase relatively low.

\subsection{Determining the next best view}\label{subtask-nbv}
This subtask aims to develop an active sensing strategy which provides a good approximation to the next best view depending on current knowledge of the scene and possibly also on previous actions.

It is essential for our system to find a next viewpoint that corresponds to a high information gain.
At some point all important views should have been used at least once to find good object candidates.
Also, the resources of energy and time are limited in our use case, therefore movements without specific intention should be avoided and the distance traveled between the viewpoints should be minimized.

We can evaluate our method by benchmarking the overall performance of the system using our method against the performance with random movements.

This subtask is the main research area of the project and must be treated as a high risk area.
It is complex, multifaceted and also the area in which we intend to do the most development of our own, as opposed to using existing libraries.
We shall mitigate this risk by keeping our initial objectives modest: we aim to implement Surmann's strategy for computing next best views by estimating information gain, and then compare the results of the method against a simple metric such as random movements.

If this objective is completed early enough, then we may extend the project by implementing another next best view algorithm; candidates for this could include building up low-level behaviours using a subsumption architecture \cite{brooks1986robust} or formulating the problem as a \textbf{P}artially \textbf{O}bservable \textbf{M}arkov \textbf{D}ecision \textbf{P}roblem (POMDP) with an information-theoretic objective function \cite{lauri2015planning}.
Exploration-based methods may be less useful to us, as our objective is to analyse a small scene in detail rather than to explore a large one.

\subsection{Fuse data of multiple views together}\label{subtask-viewfusion}
A 3D map has to be updated with depth information from the different viewpoints and from the movement in between.
To save computing time only a single image for each viewpoint is used for the actual object detection.
Some images from the movement between viewpoints might be used for updating the 3D map.

Our 2D object candidates from a single view have to be mapped to the 3D map.
Only then is it possible to form consistent 3D object candidates over multiple viewpoints.
Then the calculated object candidate estimations from different views have to be fused.

Without this method the object discovery ability of our robot won't benefit from the different views.
The 3D map is essential for the computation of the NBV.
However whlie the processes involved in this process are complex, we are fortunate that this is a common problem in robotics for which proven solutions are publicly available (see \ref{materials}).
This fact significantly mitigates the risk associated with this subtask.

\subsection{Move the robot to a desired goal point}\label{subtask-movement}
Once the next best view has been calculated, the robot needs to be able to actually move to it.
Movement control is not a focus for us, so we will use the simplest available methods to implement this.
For the table scenario, we may be able to further restrict the problem by only permitting the robot to move along a circular path around the table, as  is done in \cite{atanasov2014nonmyopic}.
However, this may prove to be detrimental to performance; this has to be evaluated during the project.
Obviously, in the scattered objects scenario this restriction cannot be sensibly enforced; this scenario also has the added complexity of introducing obstacle avoidance.

The risk in this subtask should be low, due to our very simple needs in this area.


\subsection{Evaluation of the system}\label{subtask-evaluation}
This subtask divides into two areas: optimizing the system and then evaluating it against benchmarks.

Once the method is working in general, we can search for the best parameterization for the different sub-methods used.
This is necessary, because there will be many different parameters that we have to set and it will not be obvious which values are the best especially in interaction with all aspects of our system.

When we believe we have found the best parameterization of our system, we can benchmark its performance.
Our basic evaluation will be to compare the accuracy of the object proposals  (in terms of number and pose)generated by the system when using the information gain-based next best view computation to those generated when moving the robot either randomly or not at all.
We will also look at the robot's time taken and distance traveled in order to reach a given threshold of certainty in regard to its generated proposals.

If time permits, we may implement an additional next best view algorithm and evaluate its performance in the same way.

The main risk in this subtask is that evaluating the accuracy of the generated proposals will require matching them to our ground truth model.
We will have to be aware that attempting to register the generated model against the ground truth is not a foolproof process, and may require careful manual inspection to ensure that the comparison is as accurate as possible.
Also, we will need to make sure that we create our ground truth model in such a way that it is truly comparable to the model generated by our system.

\section{Materials}\label{materials}
In this section, some details to the used hardware components and methods will be described.
\subsection{Hardware}
We will use a RGB-D camera to have both color and depth information available.
This camera will be mounted in a fixed position on a simple mobile platform robot like the TurtleBot.
We shall also need a selection of objects in order to create the scene(s) for the system to analyze.

\subsection{Software}

The main software requirement for the project is the Robot Operating System (ROS), which shall be used for managing the interaction between the different modules of the system.
It also includes several pieces of specific functionality which we shall be using; details of this are given below.

(\textbf{\ref{subtask-datageneration}})
A ground truth data model, built by taking multiple images using a static RGB-D and integrating them into a 3D model.
This model will then need to be manually labeled.\\
% To create a ground truth data set, we will have to define the spatial location and the boundaries for each object in a scene.
% For this, we can create a 3D map of the scene and label points of different objects manually.\\
(\textbf{\ref{subtask-objectdiscovery}})
The computation of object candidates for each individual view can be done with Garc\'{i}a and Frintrop's visual attention system\cite{garcia2013computational} (see section \ref{relatedwork}).\\
(\textbf{\ref{subtask-nbv}}) We shall be implementing the next best view planner described by Surmann et. al.\cite{surmann2003autonomous}.
If we reach the stage of implementing further next best view planners then we shall investigate what existing frameworks are available to us for them. \\
% Determining the next best view is probably the most difficult task as there are numerous different aspects to it.
% The NBV should for example view previously unseen parts of the scene, it should focus on areas with high uncertainty in the labeling and the pose should be close to the current one.
% An easy approach could be to define low-level behavior rules from these requirements and build a subsumption architecture \cite{brooks1986robust} from them.
% Alternatively, we could adapt the method from Surmann et al. \cite{surmann2003autonomous} described in section \ref{relatedwork}.
% Another approach we might be able to use are \textbf{P}artially \textbf{O}bservable \textbf{M}arkov \textbf{D}ecision \textbf{P}roblems (POMDPs) with a information-theoretic objective function \cite{lauri2015planning}.
% In the approach utilizing POMDPs a greedy/myopic view planner that only considers a sequence of a single next viewpoint might be sufficient. 
% Exploration driven methods like frontier-based exploration may not be a good approach for our system because we want to analyze a small area in detail, not get an overview of a bigger environment.
% We plan to test different active sensing strategies for our system.\\
(\textbf{\ref{subtask-viewfusion}})
We will use the Kinect Fusion algorithm to build a 3D map from our RGB-D images.
An open source implementation of Kinect Fusion ("KinFu"\cite{Pirovano2011}) is implemented in the Point Cloud Library.\\
% To build a 3D map we have to solve the SLAM problem, the robot has to be aware of its position and then data from multiple views can be fused together by registration.
% The sensors of the robot are not arbitrarily precise so we are not aware of its exact position. Therefore, for the registration process the geometric structure of different overlapping views has to be considered, which can be done with the Iterative Closest Points algorithm \cite{surmann2003autonomous}.
% We should be able to use the KinectFusion algorithm which provides the functionality of building a map by integrating the data of the moving depth sensor.
% To map our 2D object candidates from the color stream of the camera to a 3D map of the scene we can use the additional depth information.

% Labeling of 3D object candidates can be done as in \cite{garcia2013computational}.\\
% :
% Each voxel of the same proto-object from one view gets the same label assigned to it.
% The label that all voxels in the currently fixated proto-object get depends on the labels the contained voxels already have.
% If most voxels are unlabeled a new label is used for the proto-object.
% Otherwise the most frequent label is used.
% If one voxel is continuously assigned with the same label the confidence in that label increases, if many different labels are assigned to it then it is more likely not part of any object.\\
(\textbf{\ref{subtask-movement}}
We will communicate with the robot via ROS.
For the core project, the ROS Navigation Stack\cite{ROSWikiNavigation} package will be sufficient for controlling the robot's movement, although if we attempt the scattered objects scenario then we may need to evaluate whether a more advanced solution is necessary.

\section{Final output}\label{finaloutput}

At the end of the project, we aim to have the following core deliverables:

\begin{enumerate}
	
	\item A system, implemented on a physical mobile robot platform, which can successfully:
	\begin{itemize}
		\item perform object discovery on a scene (in the table scenario) using saliency-based object detection;
		\item calculate the optimum next best view using information gain-based NBV calculation;
		\item move to the calculated viewing point;
		\item and repeat the above until an accurate assessment of the number and position of items in the scene has been reached.
	\end{itemize}
	The final assessment should be reached within a reasonable timeframe.

	\item A report on the system, including:
	\begin{itemize}
		\item details of the algorithms implemented;
		\item evaluation of the system's performance in object discovery using the information gain-based NBV calculation in comparison to its performance when:
		\begin{itemize}
			\item not moving
			\item moving at random
		\end{itemize}
		\item discussion of the strengths and weaknesses of the system as a whole, including the hardware platform;
		\item discussion and evaluation of the design and implementation process.
	\end{itemize}

\end{enumerate}

If time permits, we may extend the project in one or more of the following ways:

\begin{itemize}
	\item By implementing another next best view algorithm and evaluating its performance in comparison to the other methods.
	\item By evaluating the system on the scattered object scenario.
\end{itemize}


\newpage
\bibliographystyle{plain}
\addcontentsline{toc}{section}{Bibliography}% Add to the TOC
\bibliography{bib}


\end{document}
