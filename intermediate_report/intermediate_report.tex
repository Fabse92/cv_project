\documentclass[a4paper,11pt,english]{article}
\usepackage[english]{babel} 
\usepackage[T1]{fontenc}    
\usepackage[utf8]{inputenc} 
\usepackage{graphicx}       
\usepackage{hyperref}      


\begin{document}

\title{Active strategies for object discovery}
\author{Phil Bradfield \and Jan Fabian Schmid}
	
\maketitle 

\section{Introduction}
Our project is to develop an autonomous mobile system for object detection.
Instead of using only single images to find positions and shapes of objects in an environment, we can use a sequence of images from different viewpoints.
It is desirable for an autonomous robot that he is able to interact and use objects in his environment. However, therefore he has to be aware of possible locations of objects, then he can try to classify objects.
For this task object detection can be used. An object detection algorithm would provide a list of object proposals that are worth to be analyzed in further detail.
Object proposals restrict the search space for useful objects.
Such restrictions are necessary, because every mobile system bears certain limitations to its capabilities. 
Mostly processing power, energy and time are limited.

Usually object detection algorithms only work on single 2D images \cite{atanasov2014nonmyopic}.
A mobile system, however, will produce a sequence of images during its process of exploring its environment.
This produces a number of additional challenges to the traditional 2D problem that we have to deal with.
One is that at each taken viewpoint a new set of object proposals is calculated, these have to be fused with previous object proposals. The second big problem is that the robot has to decide where to look next. Which is the problem of finding the Next Best View (NBV). 
For our project the goal is to analyze and evaluate multiple possible NBV-algorithms in their capability of finding good viewpoints to examine the environment.\medskip

This intermediate report is structured as follows:
The next subsection of this introductory chapter presents the three application scenarios that we want to test our system in.
Then some related work is described. The presented publications solved similar problems to the ones we encounter.
In chapter \ref{background} we will go into detail of some of the used approaches in our system.
Following, Chapter \ref{system} will provide an overview of the information flow in our system, some steps will be described in more detail. We also talk about the already done progress and about possible extensions to the system.
In the last Chapter \ref{timeline} we will try to assess in a timeline how we will proceed working on the system. 
 
\subsection{Application scenarios}
Currently three scenarios of application with different complexity are planned.
\begin{itemize}	
	\item \textbf{Table scenario}: Objects of different size, shape and color are assembled on a table. Some objects are partially or completely occluded by other objects, therefore a single viewpoint is not sufficient to detect all objects. The mobile system has a RGB-D camera mounted at an appropriate hight to look on the table. It can move freely around the table to reach different viewpoints.
	\item \textbf{Pre-recorded images scenario}: This scenario is only an intermediate application for testing during the development of the system. We utilize the same setting as in the table scenario, but without an autonomous system. Using a RGB-D camera on a tripod, we take pictures from different viewpoints of the table. The system can use these images for object detection, computation of a NBV or movement of the system is not required.
	\item \textbf{Scattered objects scenario}: For the most complex scenario objects lie scattered on the floor. The mobile system can move in between the objects to take views on different areas of the scene. This scenario is especially more demanding on the abilities of computing the NBV and moving the robot to the desired viewpoints, which should allow us to examine the pros and cons of different NBV-algorithms.
\end{itemize} 

\subsection{Related work}
In this chapter we introduce some other approaches in which subproblems we encounter ourself with our system have been solved.\medskip

García and Frintrop build a framework for 3D object detection by utilizing a visual attention system \cite{garcia2013computational}.
They work with a recorded video stream from an RGB-D camera. The color and depth stream are separately processed. 
The color stream is used to create so called proto-objects, which are areas with high probability of being part of an object. These proto-objects correspond to the peaks in the calculated saliency map. 
Simultaneously, the image is segmented into areas of pixel with similar color. 
For each proto-object the overlapping segments are considered to correspond to the proto-object. These segments are united to a object candidate.

At the same time, the depth stream is used to build a map of the scene.
By projecting the object candidates into the 3D map they are able to label voxels as associated to a certain object in the scene.
An inhibition of return (IOR) mechanism for three dimensional scenes is used to allow the algorithm to focus on one salient region after the other.

We can use this framework as a starting point for our project.
As the framework is used for pre-recorded videos, the active sensing part that we need is missing.
Apart from that, we can use the described methods for object candidate creation and the fusion of data from multiple views.
For our project it may not be necessary to implement a 3D IOR map. This is because we don't have to use a continuously moving video stream to find object candidates. In our case it is possible to stop movement at our desired viewpoints and take a picture from which object candidates are determined. \medskip

Meger et al. created with Curious George a mobile system that has been successful in the past in object recognition tasks \cite{meger2010curious}.
The robot is given a set of objects, it will then compute a visual classifier for each in a training phase.
Afterwards the robot drives autonomously through an environment. It is simultaneously exploring and searching for the known objects.
To build this map and navigate in it they have to solve the Simultaneous Localization and Mapping (SLAM) problem.
This is done with GMapping, which produces a 2D occupancy grid map from laser scan and odometry data (see Section \ref{system:slam}).
Curious George is analyzing his environment in two steps.
At first, the robot is guided through a frontier-based exploration algorithm, until a map of the whole scene is built.
Then, as a second phase, the scene is analyzed in more detail:
They use an attention system with a saliency map comparable to the one in the approach of García and Frintrop to find interesting places in the already discovered environment.
Saliency is used to focus the attention of the system to relevant parts of the environment. This is necessary as many possible views are redundant or promise only a very low information gain.
Therefore, visual saliency is used to evaluate the potential information gain of an area, supplementary the environmental structure is analyzed to find areas like desks where a high amount of objects is to be expected.

Even though the system of Meger et al. does object recognition, some parts of the system can be adopted by us.
We have to solve SLAM as well. Therefore, we will also use GMapping. Our robot has no laser range-finder to generate laser scan data, but with the depth data from Kinect we can simulate such data.
The two steps of exploring is an interesting approach that we can keep in mind for our system.
And as a first approach for a simple NBV-planner we could use a frontier-based exploration algorithm. \medskip

Li et al. presented an online system for incremental scene modeling \cite{li2016incremental}.
They combine the capabilities of a SLAM-based scene understanding framework with semantic segmentation and object pose estimation.
Their goal was to build a multi-view recognition system that is working fast and with the necessary accuracy required for robotic application scenarios.
A problem for single-view recognition systems are cluttered scenes where several objects are occluding each other.
Multiple observations of the same scene are usually available for the robot, therefore they want to use these to improve recognition abilities.
They train object models on a large set of partial views, also the model constructs a background class through a provided sequence of scene views without any objects in them.
During application a set of object hypotheses is updated with every new frame from a RGB-D camera sequence.
To find corresponding spatial areas at the different viewpoints they use the estimated current camera pose from a dense SLAM method.
In their results they reach improved performance in respect to single-view methods for semantic segmentation and object pose estimation.

In our use case the robot doesn't get object models to learn classifiers that can be used during application.
Therefore, this approach is not feasible for us.
However, their work shows the advantage of multi-view object recognition over single-view methods.
The method of fusing multiple views through knowledge of the current camera position seems to be the standard approach, which we can be apply to solve this problem in our system.

\section{Theoretical background}
\label{background}
In this section we will introduce some more general methods that we will utilize for our system.

\subsection{Saliency based object discovery}
\label{background:saliencyobjectdiscovery}
We perform 2D object discovery on RGB images with a saliency based approached.
Our method is strongly oriented on the approach of García et al. \cite{garcia2015saliency}, which is summarized in the following.

In their paper they introduce a saliency-based object discovery method on RGB-D data with a late-fusion approach, which means that color and depth object candidates are computed separately and then combined.
This method stands in contrast to early-fusion approaches in which the computation of object candidates is directly performed on the RGB-D data. As it is done in the Voxel Cloud Connectivity Segmentation \cite{papon2013voxel} for example. 

The method of García et al. consists of four steps:

\begin{enumerate}
	\item First, salient blobs in the image are calculated.
	A salient region is a region in the image with high contrast to the surrounding.
	The contrast for a specific region can be calculated in respect to different features in parallel.
	To find salient blobs their first step is to calculate a saliency map with the VOCUS2 system \cite{frintrop2015traditional}, which considers red/green, blue/yellow and intensity center-surround contrasts.
	Then the salient blobs are computed with seeded region growing on the local maxima in the saliency map.
	\item The second step, which is done simultaneously to the salient region extraction, is to segment the image.
	They use four different segmentation methods. The Felzenszwalb and Huttenlocher algorithm \cite{felzenszwalb2004efficient} that only considers color. Surface clustering searches for surface patches in the depth data. The already mentioned Voxel Cloud Connectivity Segmentation for an early-fusion approach. And as a fourth method they use the late-fusion approach of combining the candidates from color and surface segments.
	\item For candidate generation the information of salient regions and segments is brought together.
	An object candidate consists of multiple segments, which allows precise boundaries of the candidates.
	For each salient region the set of overlapping segments are selected as an object candidate.
	\item The last step is to rank the computed candidates. They compare three different ranking methods.
	The most simple approach is to rank the candidates by a score calculated as average saliency times square of the area size of the candidate.
	The second uses 3D convexity as a measure and the third considers a range of features that are learned by a SVM.
\end{enumerate}

They conclude from the comparison of the different used methods that the late-fusion approach performs best.
The SVM for candidate ranking has a much higher precision when only considering a few object candidates; however, the performance of the three different approaches is more similar for recall and it converges with a higher number of object candidates.

\subsection{Surmann approach}
\label{background:surmann}
Surmann et al. developed a system for autonomous digitalization of 3D indoor environments \cite{surmann2003autonomous}.
This system consists of three core modules: A mapping algorithm, a view planner and a navigator to move the robot to a desired goal.
Especially interesting for us is how they compute a NBV.
A view planner is necessary in the  scenario, because many possible views promise only a very low information gain and the amount of views that can be used is limited by time and energy constraints.
An visualization of the steps of their method can be seen in Figure \ref{fig:surmann}.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1\textwidth]{src/nbv.png}
		\caption{Next best view planning as it is done in the approach of Surmann et al.}
		\label{fig:surmann}
	\end{center}
\end{figure}

To compute the next best view they generate multiple horizontal 2D layers of their 3D map of the environment.
An example of one of these layers is pictured in the first frame in Figure \ref{fig:surmann}.
The green crosses represent single data points of the laser range finder.
They use the Hough transform to find lines in the layer which represent the obstacles (e.g. walls) they already detected (frame 2).
The lines are then connected to form an estimate of the layout of the room (frame 3).
In frame 4 it is shown how the connections between lines are calculated on the example of the line between lines 1 and 2. 
The endpoints of the lines are sorted with increasing polar angle counterclockwise.
Lines with neighboring endpoints in that sorting are then connected.
The next step is to randomly generate possible viewpoints in the already discovered space, which are pictured as green squares in frame 5.
For each viewpoint $p$ the information gain $V(p)$ is calculated as a measure of how many unseen/assumed lines can be seen from it.
A compromise between traveling distance and information gain is then used to decide on the next viewpoint.
The score $S(p)$ for the viewpoint $p$ is calculated as $S(p) = V(p) exp(-c_1||r-p||)exp(-c_2||\Theta_r-\Theta_p||)$, where r is the position of the robot and $\Theta_r$ its current orientation, p the position of the viewpoint and $\Theta_p$ the orientation of the viewpoint. The terms can be weighted with the constants $c_1$ and $c_2$.

\subsection{TODO Phil: Subsumption architecture}
\label{background:subsumption}

\section{System description and progress}
\label{system}
A general overview of our system is given in figure \ref{fig:overview}.

\begin{figure}
	\begin{center}
		\includegraphics[width=1\linewidth]{dot/overview.png} 
		\caption{Overview of the system}
		\label{fig:overview}
	\end{center}
\end{figure}

Before we start explaining the graph in detail, it is useful to have a look at the available input and the desired output of our system.
\begin{itemize}
	\item \textbf{Input:}
	\begin{itemize}
		\item We will use a Turtlebot as mobile platform, which provides odometry data from the motion sensors.
		\item Onto the Turtlebot a Kinect is mounted, which provides RGB-D images of the scene.
	\end{itemize}
	\item \textbf{Output:}
	\begin{itemize}
		\item During operation of the system it is moving around the scene. Therefore, motor control can be seen as intermediate output of the system.
		\item The goal is to retrieve a 3D map with incorporated labels of all found object proposals.
	\end{itemize}  
\end{itemize}
Independent of the used application scenario the robot starts with no available knowledge of the scene.
Therefore, we are completely dependent on the system input to infer information.
At the beginning only a RGB-D image of the robot viewpoint at the start is available.

One processing line is now using the RGB image to find any possible objects in the view, this is described in detail in section \ref{system:obj_discovery}.
Simultaneously the depth image of the Kinect camera is processed.
The depth data is used to simulate a laser scanner, which provides depth data of one horizontal line in the robot view. 
This laser scan data is used together with the odometry data to solve the Simultaneous Localization And Mapping (SLAM) problem. This part is described in more detail in section \ref{system:slam}.
The SLAM-algorithm provides a 2D occupancy grid map, which is a kind of floor plan, containing information about obstacles and free space in the already discovered environment. A second output of SLAM is an estimate of the current robot pose.
This pose is used together with the new found object proposals of the current view and the current depth image to update the 3D map that contains a labeling of voxels to object proposals. This procedure is specified in section \ref{system:fusion}. 
The next step is to compute a next best viewpoint for the robot that should maximize the information gain about the environment.
For this computation all data that has been gathered to this point can be used, particularly the positions of already found object proposals and the current floor map will be used for this step. Details can be found in section \ref{system:nbv}.
Once a desired view point is calculated, it is used together with the current occupancy grid map and pose estimation to plan movement of the robot to this point in space.
This part is described in section \ref{system:navigation}.
The output of the navigation module is the motor control to move the system.
During movement the SLAM algorithm is constantly processing the camera data to update the map and keep track of the robot position.
Once the viewpoint is reached the next iteration of the described process is done.

After describing the individual parts of our system in more detail, the last subsection of this chapter provides a list of possible extensions to the system, that we will work on if time permits.

\subsection{Object discovery}
\label{system:obj_discovery}
To find object candidates, we use a part of the method from García et al. described in Section \ref{background:saliencyobjectdiscovery}.
For simplicity we will only use color candidates; the use of depth candidates that are then fused with the color candidates is a possible extension for our project.
Our object discovery works as follows: First we calculate a saliency map with VOCUS2 \cite{frintrop2015traditional}, then we iterate over local maxima in the map to calculate salient blobs with seeded region growing.
That means that all pixels neighboring the current local maximum are considered as part of the salient region if their saliency value is at least x\% or more of the maximum.
This process is done once with 60\% and once with 70\% similarity requirement.
A salient region is only kept if they have at least a value of 120 (from a maximum of 255) mean saliency to only keep actually interesting regions.
Some of the salient regions will overlap each other, if the overlap is greater than 50\% only the one with higher saliency score is kept, which is calculated as mean saliency times square root of the area of the corresponding region.
The segmentation is done with a C++ implementation of the Felzenszwalb and Huttenlocher algorithm \cite{felzenszwalb2004efficient} by Christoffer Holmstedt \footnote{\url{https://github.com/christofferholmstedt/opencv-wrapper-egbis}}.
For each remaining salient region a object candidate is now calculated by finding all segments that overlap to at least 30\% with the region.

This part of our system is already fully implemented and working.

\subsection{Solving SLAM}
\label{system:slam}
To solve the simultaneous localization and mapping we use a ROS wrapper for OpenSlam's GMapping algorithm \footnote{\url{http://wiki.ros.org/gmapping}}, which is an implementation of Fast SLAM.
It uses laser scan and odometry data to compute a 2D occupancy grid map of the discovered environment.
The algorithm requires laser scan data of a horizontally-mounted laser range-finder.
We don't have such a laser range-finder; however, we can transform the depth image provided by the Kinect camera to laser scan data.
Therefore we have to define the height for which the horizontal laser range data is computed.
Because the data is used for navigation the height should be at ground level.

The ROS wrapper provides topics to retrieve the build 2D occupancy map and the current pose estimation of the robot in this map.

\subsection{TODO Phil: Fuse data of multiple views together}
\label{system:fusion}

\subsection{Determining the next best view}
\label{system:nbv}
After processing the image data from one viewpoint the robot has to decide where to go next.
This step is the core of our system and our main research focus.
The goal is to find as many and as precise as possible object candidates.
The computation of the NBV determines the strategy how to reach this goal.

Desirable properties of the NBV algorithm are:
\begin{itemize}
	\item The computed NBVs should maximize the information gain on the scene. It is not necessary to take the same or similar viewpoints multiple times. Also the view should focus on the region of interest containing objects. If the remaining energy of the robot has a value associated with it, at some point the predicted information gain has less value then the required energy to reach the NBV. At this point our system can terminate.
	\item The computational cost should be reasonable. Because of the constraints of energy and time in mobile systems the value of a very good NBV against an mediocre NBV has to be pondered against the additional effort to compute the better NBV. It is probably not appropriate if the energy consumption for calculating a very good NBV and moving to it is as high as calculating and moving to two mediocre NBVs. This is especially important in our use case, because exploring the whole scene is already the main goal of the NBV.
\end{itemize}

We want to implement and evaluate multiple NBV-algorithms during our project.
Therefore, we will use the following metrics:
\begin{itemize}
	\item Intersection over Union (IoU). We calculate the percentage of the surface area of the object proposals that has been correctly classified as part of an object.
	\item How many of the individual objects in the scene have been identified, which means that it has an individual label assigned to it.
	\item Time and energy consumption until termination
\end{itemize}

In the following subsections we will explain in detail the different NBV algorithms that we plan to implement.

\subsubsection{Frontier-Based Exploration}
This approach very simple without much computation going into the NBV.
The goal of this method is to find viewpoints that are suitable to explore the whole scene, which is something an autonomous robot has to do in a new environment anyhow.
In this method no additional effort is taken to focus on objects in the environment, objects are discovered as a side-product of the exploration process.
The performance of this simple approach will be used as baseline to evaluate the gain of more sophisticated approaches.

In Frontier-Based Exploration the robot is always moving to the nearest reachable frontier.
A frontier is a border region between explored free space and unexplored space.
A frontier is reachable if a path from the current robot position through the known free space to the frontier position exists.

Frontier based exploration has been implemented in a ROS package \footnote{\url{http://wiki.ros.org/frontier_exploration}} that we should be able to use.

\subsubsection{TODO Phil: Subsumption Architecture}
Another baseline approach based on behavior robotics.

\subsubsection{Surmann approach}
This NBV method is based on the approach described in Section\ref{background:surman}.
The basic functionality of this approach is to explore an indoor environment.
As NBV this approach samples the already discovered environment for the nearest viewpoint which allows a view on as many unseen areas (lines) as possible.
Exploration is an important aspect that has to be considered by our NBV planner, because the robot can only find all objects when it has at least looked in all general areas of the room once.
In our system we a occupancy grid map is computed for navigation purposes, to use the Surmann approach, we compute another 2D grid map for the height at which the objects are located.
The whole method is then performed on this 2D grid map.

We adapt the process at the point of calculating the information gain for all samples.
For example, a high information gain can be expected to appear when looking at a region with high uncertainty of the object labels.
This might be a region that is close to two objects and the system couldn't figure out to which object this region belongs.
Further adaptations to the information gain measure could be to extrapolate from the already found object surfaces. 
For example the system could predict that the surface won't abruptly change its outlines and therefore a high possibility exists for continuation of the object into a certain region that has not already been viewed.

We will test out different adaptations to the information gain measure. This part is not implemented yet.

\subsection{TODO: Navigation}
\label{system:navigation}
\subsection{Extensions}
\label{system:extensions}
In the following list we present possible extension to the core system that has been described so far in this chapter. 
The extensions could improve the system, but they shouldn't be essential for the performance of it.

\begin{itemize}
	\item Include depth information for the computation of object proposals.
	As described in Section \ref{system:obj_discovery} we only use color candidates, but the work of García et al. we summarized in Section \ref{background:saliencyobjectdiscovery} showed that the additional usage of depth information that is integrated in a late-fusion approach can significantly improve the accuracy of saliency based object discovery.
	\item An additional NBV planning algorithm that exploits POMDPs.
\end{itemize}

\section{Timeline}
\label{timeline}

\newpage
\bibliographystyle{plain}
\addcontentsline{toc}{section}{Bibliography}% Add to the TOC
\bibliography{bib}

\end{document}
