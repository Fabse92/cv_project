# Candidate Locator package

## Usage

Consists of a service client (`client_locator_client`) and a ROS node (`client_locator_node`). The node is run like any other ROS node and mostly just sits there waiting, monitoring tf and camera_info while waiting for a snapshot to arrive. To run the node you will need:

- Terminal window 1: `roscore`
- Terminal window 2: `roslaunch p3dx_description everything.launch`
- Terminal window 3: `roslaunch candidate_locator candidate_locator.launch`

To trigger the generation of object candidates, you then run the service client. The client calls the snapshot generator service and then publishes the receieved snapshot as a message of type `object_candidates::SnapshotMsg` on the topic `/candidates_snapshot`. 

- Terminal window 4: `./(catkin_ws)/devel/lib/candidate_locator/candidate_locator_client`

The node listens to the `/candidates_snapshot` topic and processes new messages in the callback. It publishes the results to the topic `candidate_point_clouds` as a message of type `candidate_locator::ArrayPointClouds`, which is an array wrapper around `sensor_msgs::PointCloud2` (exactly like how `object_candidates::ArrayImages` wraps around `sensor_msgs::Image`).

Currently, for visualisation/debug purposes, I'm also creating another point cloud containing all the candidate surfaces. This is published to the topic `/candidate_pcs_debug`. This topic can be visualised in rviz.

## Todo

The point clouds generated by the node are reasonable in terms of their size, shape, and orientation, but the full localisation procedure is not yet implemented. In particular:

- **Todo 1:** `transformPointTo3dRay` returns a 3D point which defines a vector in the direction of the pixel, with z = 1.0. At the moment, I'm just normalising this point and shoving it straight into the point cloud so that the node has something to publish, but that ignores the actual distance to the object, and hence the localisation is rubbish. To get the distance to the object, we need (I think) to multiply it by this normalised point by the distance given in the depth image, but I'm getting really weird numbers in the depth image which I don't understand: I've read in several places that the numbers in /depth/image_raw are distances in millimetres, but the numbers I'm seeing are huge so I'm not sure what's going on with that.

Also I don't think the two points below are an issue while using Gazebo (because as far as I can tell, the OpenNI plugin for Gazebo simulates the RGB and depth cameras as being located in the same place and thus sharing a frame) but we will need to bear them in mind on the real robot where we will have separate frames for the two cameras:

- **Todo 2:** We need the both the depth image and the candidate binary images to be rectified in order (a) for transformPointTo3dRay to work properly and (b) to be able to perform the multiplication in Todo 1. Thus, when moving to the physical robot, we need to make sure that the images going into the snapshot are rectified.
- **Todo 3:** We'll need to take care that we differentiate between the frames for the RGB and depth cameras and use the right on at the right time.


Other thoughts:

- Should I be timestamping the output point clouds with the timestamp of the original snapshot, so that we have a link between the two? Or is that just going to confuse the whole message passing system?
- It would be nice to be able to visualise all candidate point clouds in rviz. I guess this would require writing an rviz plugin to process messages of type `candidate_locator::ArrayPointClouds`... which sounds like potentially significant effort. Nice to have, but low priority for now.

## A bit more detail

When `candidate_locator_node` is run, an object of type `candidate_locator::CandidateLocator` is generated. This is where all the heavy lifting happens. In "idle" mode (i.e. when not actively processing candidates) `CandidateLocator` does four things:

1. Maintains a `tf::tf_listener` which monitors and maintains a cache of the messages broadcast over `/tf`.  The cache is ten seconds long by default, we can increase this if necessary.
2. Subscribes to the topic `/camera/depth/image_raw` using `ImageTransport`'s `subscribeCamera` method. This gets the `sensor_msgs::CameraInfo` object that we need to create an `image_geometry::PinholeCameraModel` object, which is what we will later use for the projection of image points into 3D. The actual depth image itself is ignored.
3. Subscribes to the topic `/candidates_snapshot`, where snapshots will arrive.
3. Advertises the topic `/candidate_point_clouds`, which is where the located point clouds will be published to. Currently, for debugging/visualisation, it also advertises `/candidate_pcs_debug`, which is the first candidate point cloud output in an rviz-friendly message format. 

When a message is received on `/candidates_snapshot`, the `CandidateLocator::candidatesCallback` method is called. This method performs the following steps:

- Gets the transform from `/camera_optical_frame` to `/map` from the `tf_listener` using the timestamp from the snapshot's depth image.
- Creates a `candidate_locator::ArrayPointClouds` message object
- For each binary candidate image:
  - Creates a `pcl::PointCloud` object
  - For each pixel in the candidate image:
    - Transforms the pixel to 3D coordinates in the camera frame using `image_geometry`
    - Adds the 3D point to the point cloud
  - Converts the PCL point cloud object to a ROS `sensor_msgs::PointCloud2` message object
  - Transforms the ROS message from the camera frame to the map frame.
  - Adds the point cloud the the point cloud array
- Publishes the point cloud array to `/candidate_point_clouds`
