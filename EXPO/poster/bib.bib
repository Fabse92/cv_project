% Encoding: UTF-8
@article{atanasov2014nonmyopic,
  title={Nonmyopic view planning for active object classification and pose estimation},
  author={Atanasov, Nikolay and Sankaran, Bharath and Le Ny, Jerome and Pappas, George J and Daniilidis, Kostas},
  journal={IEEE Transactions on Robotics},
  volume={30},
  number={5},
  pages={1078--1090},
  year={2014},
  publisher={IEEE}
}

@inproceedings{garcia2013computational,
  title={{A Computational Framework for Attentional 3D Object Detection}},
  author={Garc{\'i}a, Germ{\'a}n Mart{\'i}n and Frintrop, Simone},
  booktitle={Proc. of the Annual Conf. of the Cognitive Science Society},
  year={2013}
}

@inproceedings{meger2010curious,
  title={Curious george: An integrated visual search platform},
  author={Meger, David and Muja, Marius and Helmer, Scott and Gupta, Ankur and Gamroth, Catherine and Hoffman, Tomas and Baumann, Matthew and Southey, Tristram and Fazli, Pooyan and Wohlkinger, Walter and others},
  booktitle={Computer and Robot Vision (CRV), 2010 Canadian Conference on},
  pages={107--114},
  year={2010},
  organization={IEEE}
}

@INPROCEEDINGS{garcia2015saliency,
author={G. M. García and E. Potapova and T. Werner and M. Zillich and M. Vincze and S. Frintrop},
booktitle={2015 IEEE International Conference on Robotics and Automation (ICRA)},
title={{Saliency-based object discovery on RGB-D data with a late-fusion approach}},
year={2015},
volume={},
number={},
pages={1866-1873},
keywords={image colour analysis;image fusion;image segmentation;image sequences;object recognition;RGB-D data;clutter;color segmentation algorithms;depth segmentation algorithms;indoor scenes;late-fusion approach;object candidates ranking;saliency-based object discovery;segment extraction;sequence;Color;Image color analysis;Image segmentation;Proposals;Robots;Support vector machines;Three-dimensional displays},
doi={10.1109/ICRA.2015.7139441},
ISSN={1050-4729},
month={May},}

@inproceedings{papon2013voxel,
  title={Voxel cloud connectivity segmentation-supervoxels for point clouds},
  author={Papon, Jeremie and Abramov, Alexey and Schoeler, Markus and Worgotter, Florentin},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2027--2034},
  year={2013}
}

@InProceedings{frintrop2015traditional,
author = {Frintrop, Simone and Werner, Thomas and Martin Garcia, German},
title = {{Traditional Saliency Reloaded: A Good Old Model in New Shape}},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}


@Article{felzenszwalb2004efficient,
author="Felzenszwalb, Pedro F.
and Huttenlocher, Daniel P.",
title={{Efficient Graph-Based Image Segmentation}},
journal="International Journal of Computer Vision",
year="2004",
month="Sep",
day="01",
volume="59",
number="2",
pages="167--181",
abstract="This paper addresses the problem of segmenting an image into regions. We define a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image. We then develop an efficient segmentation algorithm based on this predicate, and show that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties. We apply the algorithm to image segmentation using two different kinds of local neighborhoods in constructing the graph, and illustrate the results with both real and synthetic images. The algorithm runs in time nearly linear in the number of graph edges and is also fast in practice. An important characteristic of the method is its ability to preserve detail in low-variability image regions while ignoring detail in high-variability regions.",
issn="1573-1405",
doi="10.1023/B:VISI.0000022288.19776.77",
url="https://doi.org/10.1023/B:VISI.0000022288.19776.77"
}

@article{surmann2003autonomous,
title = {{An autonomous mobile robot with a 3D laser range finder for 3D exploration and digitalization of indoor environments}},
journal = "Robotics and Autonomous Systems",
volume = "45",
number = "3",
pages = "181 - 198",
year = "2003",
issn = "0921-8890",
doi = "https://doi.org/10.1016/j.robot.2003.09.004",
url = "http://www.sciencedirect.com/science/article/pii/S0921889003001556",
author = "Hartmut Surmann and Andreas Nüchter and Joachim Hertzberg",
keywords = "Autonomous mobile robots",
keywords = "3D laser range finder",
keywords = "Scan matching",
keywords = "Next best view planning",
keywords = "3D digitalization",
keywords = "3D gaging",
keywords = "Robot relocalization",
keywords = "SLAM"
}

@inproceedings{li2016incremental,
  title={Incremental scene understanding on dense SLAM},
  author={Li, Chi and Xiao, Han and Tateno, Keisuke and Tombari, Federico and Navab, Nassir and Hager, Gregory D},
  booktitle={Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on},
  pages={574--581},
  year={2016},
  organization={IEEE}
}

@article{brooks1986,
author={R. Brooks},
journal={IEEE Journal on Robotics and Automation},
title={A robust layered control system for a mobile robot},
year={1986},
volume={2},
number={1},
pages={14-23},
keywords={Hierarchical systems;Robots, locomotion;Robustness;Acoustic sensors;Boundary conditions;Control systems;Infrared sensors;Laboratories;Mobile robots;Robot control;Robot sensing systems;Robotics and automation;Robust control},
doi={10.1109/JRA.1986.1087032},
ISSN={0882-4967},
month={Mar},}

@ARTICLE{grisetti2007,
author={G. Grisetti and C. Stachniss and W. Burgard},
journal={IEEE Transactions on Robotics},
title={{Improved Techniques for Grid Mapping With Rao-Blackwellized Particle Filters}},
year={2007},
volume={23},
number={1},
pages={34-46},
keywords={SLAM (robots);mobile robots;particle filtering (numerical methods);Rao-Blackwellized particle filters;grid mapping;mobile robots;particle depletion;simultaneous localization and mapping problem;Computer science;Contracts;Distributed computing;Mobile robots;Orbital robotics;Particle filters;Proposals;Robot sensing systems;Simultaneous localization and mapping;Uncertainty;Adaptive resampling;Rao-Blackwellized particle filter (RBPF);improved proposal;motion model;simultaneous localization and mapping (SLAM)},
doi={10.1109/TRO.2006.889486},
ISSN={1552-3098},
month={Feb},}

@INPROCEEDINGS{yamauchi1997frontier,
author={B. Yamauchi},
booktitle={Computational Intelligence in Robotics and Automation, 1997. CIRA'97., Proceedings., 1997 IEEE International Symposium on Computational Intelligence},
title={{A frontier-based approach for autonomous exploration}},
year={1997},
volume={},
number={},
pages={146-151},
keywords={intelligent control;laser beam applications;mobile robots;path planning;probability;sonar;autonomous exploration;evidence grids;frontier detection;laser-limited sonar;mobile robot;navigation;office environments;path planning;probability;specular reflections;Artificial intelligence;Humans;Indoor environments;Laboratories;Mobile robots;Orbital robotics;Sonar navigation;Space exploration;Testing},
doi={10.1109/CIRA.1997.613851},
ISSN={},
month={Jul},}

@INPROCEEDINGS{newcombe2011kinectfusion,
author={R. A. Newcombe and S. Izadi and O. Hilliges and D. Molyneaux and D. Kim and A. J. Davison and P. Kohi and J. Shotton and S. Hodges and A. Fitzgibbon},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality},
title={{KinectFusion: Real-time dense surface mapping and tracking}},
year={2011},
volume={},
number={},
pages={127-136},
keywords={Cameras;Image reconstruction;Iterative closest point algorithm;Real time systems;Simultaneous localization and mapping;Surface reconstruction;Three dimensional displays;AR;Dense Reconstruction;Depth Cameras;GPU;Real-Time;SLAM;Tracking;Volumetric Representation},
doi={10.1109/ISMAR.2011.6092378},
ISSN={},
month={Oct},}

@article{meagher1982octrees,
title = {{Geometric modeling using octree encoding}},
journal = "Computer Graphics and Image Processing",
volume = "19",
number = "2",
pages = "129 - 147",
year = "1982",
issn = "0146-664X",
doi = "https://doi.org/10.1016/0146-664X(82)90104-6",
url = "http://www.sciencedirect.com/science/article/pii/0146664X82901046",
author = "Donald Meagher"
}

@Article{hornung13octomap,
author="Hornung, Armin
and Wurm, Kai M.
and Bennewitz, Maren
and Stachniss, Cyrill
and Burgard, Wolfram",
title={{OctoMap: an efficient probabilistic 3D mapping framework based on octrees}},
journal="Autonomous Robots",
year="2013",
month="Apr",
day="01",
volume="34",
number="3",
pages="189--206",
abstract="Three-dimensional models provide a volumetric representation of space which is important for a variety of robotic applications including flying robots and robots that are equipped with manipulators. In this paper, we present an open-source framework to generate volumetric 3D environment models. Our mapping approach is based on octrees and uses probabilistic occupancy estimation. It explicitly represents not only occupied space, but also free and unknown areas. Furthermore, we propose an octree map compression method that keeps the 3D models compact. Our framework is available as an open-source C++ library and has already been successfully applied in several robotics projects. We present a series of experimental results carried out with real robots and on publicly available real-world datasets. The results demonstrate that our approach is able to update the representation efficiently and models the data consistently while keeping the memory requirement at a minimum.",
issn="1573-7527",
doi="10.1007/s10514-012-9321-0",
url="https://doi.org/10.1007/s10514-012-9321-0"
}

@ARTICLE{calli2015benchmarking,
author={B. Calli and A. Walsman and A. Singh and S. Srinivasa and P. Abbeel and A. M. Dollar},
journal={IEEE Robotics Automation Magazine},
title={{Benchmarking in Manipulation Research: Using the Yale-CMU-Berkeley Object and Model Set}},
year={2015},
volume={22},
number={3},
pages={36-52},
keywords={benchmark testing;learning (artificial intelligence);planning (artificial intelligence);robots;RGB-D scans;Yale-CMU-Berkeley model set;Yale-CMU-Berkeley object set;associated database;benchmarking;geometric models;high-resolution red green blue plus depth scans;learning;manipulation tests;mechanical design;physical properties;planning software platforms;robotic manipulation research;Benchmark testing;Data models;Databases;Object detection;Prosthetics;Robots;Solid modeling},
doi={10.1109/MRA.2015.2448951},
ISSN={1070-9932},
month={Sept},}

@Article{monica2017,
author="Monica, Riccardo
and Aleotti, Jacopo",
title={{Contour-based next-best view planning from point cloud segmentation of unknown objects}},
journal="Autonomous Robots",
year="2017",
month="Feb",
day="06",
abstract="A novel strategy is presented to determine the next-best view for a robot arm, equipped with a depth camera in eye-in-hand configuration, which is oriented to autonomous exploration of unknown objects. Instead of maximizing the total size of the expected unknown volume that becomes visible, the next-best view is chosen to observe the border of incomplete objects. Salient regions of space that belong to the objects are detected, without any prior knowledge, by applying a point cloud segmentation algorithm. The system uses a Kinect V2 sensor, which has not been considered in previous works on next-best view planning, and it exploits KinectFusion to maintain a volumetric representation of the environment. A low-level procedure to reduce Kinect V2 invalid points is also presented. The viability of the approach has been demonstrated in a real setup where the robot is fully autonomous. Experiments indicate that the proposed method enables the robot to actively explore the objects faster than a standard next-best view algorithm.",
issn="1573-7527",
doi="10.1007/s10514-017-9618-0",
url="https://doi.org/10.1007/s10514-017-9618-0"
}

@ARTICLE{endres2014rgbdslam,
author={F. Endres and J. Hess and J. Sturm and D. Cremers and W. Burgard},
journal={IEEE Transactions on Robotics},
title={{3-D Mapping With an RGB-D Camera}},
year={2014},
volume={30},
number={1},
pages={177-187},
keywords={feature extraction;image colour analysis;image motion analysis;image sensors;mobile robots;robot vision;3-D mapping;Microsoft Kinect;RGB-D camera;RGB-D sensors;detailed 3-D models;domestic robots;fast camera motions;feature descriptor;feature-poor environments;flying robots;free-hand reconstruction;online operation;open source;quadrocopters;robotics community;vacuum cleaners;validation methods;visual features;Cameras;Estimation;Robot vision systems;Simultaneous localization and mapping;Visualization;Localization;RGB-D;mapping;open source;simultaneous localization and mapping (SLAM)},
doi={10.1109/TRO.2013.2279412},
ISSN={1552-3098},
month={Feb},}

@INPROCEEDINGS{blodow2011autonomous,
author={N. Blodow and L. C. Goron and Z. C. Marton and D. Pangercic and T. Rühr and M. Tenorth and M. Beetz},
booktitle={2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
title={{Autonomous semantic mapping for robots performing everyday manipulation tasks in kitchen environments}},
year={2011},
volume={},
number={},
pages={4263-4270},
keywords={Cameras;Robots},
doi={10.1109/IROS.2011.6094665},
ISSN={2153-0858},
month={Sept},}

@Article{delmerico2017nbvsurvey,
author="Delmerico, Jeffrey
and Isler, Stefan
and Sabzevari, Reza
and Scaramuzza, Davide",
title={{A comparison of volumetric information gain metrics for active 3D object reconstruction}},
journal="Autonomous Robots",
year="2017",
month="Apr",
day="22",
abstract="In this paper, we investigate the following question: when performing next best view selection for volumetric 3D reconstruction of an object by a mobile robot equipped with a dense (camera-based) depth sensor, what formulation of information gain is best? To address this question, we propose several new ways to quantify the volumetric information (VI) contained in the voxels of a probabilistic volumetric map, and compare them to the state of the art with extensive simulated experiments. Our proposed formulations incorporate factors such as visibility likelihood and the likelihood of seeing new parts of the object. The results of our experiments allow us to draw some clear conclusions about the VI formulations that are most effective in different mobile-robot reconstruction scenarios. To the best of our knowledge, this is the first comparative survey of VI formulation performance for active 3D object reconstruction. Additionally, our modular software framework is adaptable to other robotic platforms and general reconstruction problems, and we release it open source for autonomous reconstruction tasks.",
issn="1573-7527",
doi="10.1007/s10514-017-9634-0",
url="https://doi.org/10.1007/s10514-017-9634-0"
}

@InProceedings{doumanoglou2016,
author = {Doumanoglou, Andreas and Kouskouridas, Rigas and Malassiotis, Sotiris and Kim, Tae-Kyun},
title = {{Recovering 6D Object Pose and Predicting Next-Best-View in the Crowd}},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@article{xu2016,
 author = {Xu, Kai and Shi, Yifei and Zheng, Lintao and Zhang, Junyu and Liu, Min and Huang, Hui and Su, Hao and Cohen-Or, Daniel and Chen, Baoquan},
 title = {{3D Attention-driven Depth Acquisition for Object Identification}},
 journal = {ACM Trans. Graph.},
 issue_date = {November 2016},
 volume = {35},
 number = {6},
 month = nov,
 year = {2016},
 issn = {0730-0301},
 pages = {238:1--238:14},
 articleno = {238},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/2980179.2980224},
 doi = {10.1145/2980179.2980224},
 acmid = {2980224},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D acquisition, attention-based model, depth camera, next-best-view, object identification, shape classification},
} 

